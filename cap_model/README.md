# Card Approval Prediction - Model Development

This directory contains all machine learning model development code for credit card approval prediction.

---

## ğŸ“ Directory Structure

```
cap_model/
â”œâ”€â”€ data/                   # Data storage and management
â”‚   â”œâ”€â”€ raw/               # Original, immutable data
â”‚   â”œâ”€â”€ processed/         # Cleaned and transformed data
â”‚   â”œâ”€â”€ features/          # Feature engineering outputs
â”‚   â””â”€â”€ external/          # External datasets (e.g., credit bureau data)
â”‚
â”œâ”€â”€ notebooks/             # Jupyter notebooks for exploration
â”‚   â”œâ”€â”€ 01_eda.ipynb      # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 03_baseline_models.ipynb
â”‚   â””â”€â”€ 04_model_comparison.ipynb
â”‚
â”œâ”€â”€ src/                   # Source code for model development
â”‚   â”œâ”€â”€ data/             # Data loading and processing
â”‚   â”œâ”€â”€ features/         # Feature engineering pipeline
â”‚   â”œâ”€â”€ models/           # Model training and evaluation
â”‚   â”œâ”€â”€ utils/            # Utility functions
â”‚   â””â”€â”€ config/           # Configuration files
â”‚
â”œâ”€â”€ models/                # Trained model artifacts
â”‚   â”œâ”€â”€ baseline/         # Baseline models
â”‚   â”œâ”€â”€ experimental/     # Experimental models
â”‚   â””â”€â”€ production/       # Production-ready models
â”‚
â”œâ”€â”€ experiments/           # MLflow experiment tracking
â”‚   â””â”€â”€ configs/          # Experiment configurations
â”‚
â”œâ”€â”€ tests/                 # Unit and integration tests
â”‚   â”œâ”€â”€ test_data/
â”‚   â”œâ”€â”€ test_features/
â”‚   â””â”€â”€ test_models/
â”‚
â”œâ”€â”€ scripts/               # Executable scripts
â”‚   â”œâ”€â”€ train.py          # Model training script
â”‚   â”œâ”€â”€ evaluate.py       # Model evaluation script
â”‚   â””â”€â”€ predict.py        # Batch prediction script
â”‚
â”œâ”€â”€ outputs/               # Training outputs
â”‚   â”œâ”€â”€ figures/          # Plots and visualizations
â”‚   â”œâ”€â”€ reports/          # Analysis reports
â”‚   â””â”€â”€ metrics/          # Evaluation metrics
â”‚
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ setup.py              # Package setup
â””â”€â”€ README.md             # This file
```

---

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
cd cap_model

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Data Preparation

```bash
# Place raw data in data/raw/
# Run preprocessing
python scripts/preprocess_data.py
```

### 3. Exploratory Analysis

```bash
# Launch Jupyter
jupyter notebook notebooks/01_eda.ipynb
```

### 4. Train Models

```bash
# Train baseline model
python scripts/train.py --config experiments/configs/baseline.yaml

# Train with MLflow tracking
python scripts/train.py --config experiments/configs/xgboost.yaml --track-mlflow
```

---

## ğŸ“Š Model Development Workflow

### Phase 1: Data Understanding
1. Load and explore raw data
2. Understand feature distributions
3. Identify missing values and outliers
4. Analyze target variable (approval/rejection rate)

### Phase 2: Feature Engineering
1. Handle missing values
2. Encode categorical variables
3. Create derived features
4. Feature scaling and normalization
5. Feature selection

### Phase 3: Model Training
1. Train baseline models (Logistic Regression)
2. Train tree-based models (Random Forest, XGBoost)
3. Train neural networks
4. Hyperparameter tuning
5. Cross-validation

### Phase 4: Model Evaluation
1. Accuracy, Precision, Recall, F1-Score
2. ROC-AUC curve
3. Confusion matrix
4. Feature importance
5. Model interpretability (SHAP values)

### Phase 5: Model Selection
1. Compare model performance
2. Consider business constraints
3. Evaluate model fairness
4. Select production model

---

## ğŸ”¬ Experiment Tracking with MLflow

All experiments are tracked in MLflow:

```python
import mlflow

mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("card-approval-classification")

with mlflow.start_run(run_name="xgboost-v1"):
    # Train model
    model = train_xgboost(X_train, y_train)
    
    # Log parameters
    mlflow.log_params(params)
    
    # Log metrics
    mlflow.log_metrics(metrics)
    
    # Log model
    mlflow.sklearn.log_model(model, "model")
```

---

## ğŸ“ˆ Model Performance Targets

| Metric | Baseline | Target | Production |
|--------|----------|--------|------------|
| Accuracy | 0.70 | 0.85 | 0.90 |
| Precision | 0.65 | 0.80 | 0.85 |
| Recall | 0.60 | 0.75 | 0.80 |
| F1-Score | 0.62 | 0.77 | 0.82 |
| ROC-AUC | 0.75 | 0.88 | 0.92 |

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run specific test
pytest tests/test_models/test_training.py

# Run with coverage
pytest --cov=src tests/
```

---

## ğŸ“ Documentation

- [Data Dictionary](docs/data_dictionary.md)
- [Feature Engineering](docs/feature_engineering.md)
- [Model Architecture](docs/model_architecture.md)
- [Evaluation Metrics](docs/evaluation_metrics.md)

---

## ğŸ”— Related Documentation

- Main project: `/docs/README.md`
- MLflow guide: `/docs/05_MLflow_Model_Development.md`
- Deployment: `/docs/03_Helm_Deployment.md`
